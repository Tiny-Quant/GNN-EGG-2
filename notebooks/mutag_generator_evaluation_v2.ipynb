{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUTAG Generator Evaluation\n",
    "\n",
    "This notebook compiles a comprehensive evaluation of the initial class-conditional generators trained on the MUTAG dataset. It aggregates metrics from the explainee classifier, the SimGNN distance approximator, and the generators themselves to help diagnose explanation quality and training health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook goals\n",
    "\n",
    "* Load the MUTAG explainee, distance model, and per-class generators.\n",
    "* Evaluate the explainee's predictive performance on a hold-out split.\n",
    "* Measure SimGNN graph edit distance (GED) approximation quality.\n",
    "* Score each generator with respect to its loss terms, fidelity, and diversity.\n",
    "* Visualize generated graphs alongside real samples from the dataset.\n",
    "* Summarize parameter counts and other diagnostics useful for debugging.\n",
    "\n",
    "> **Tip:** Run the end-to-end training pipeline (`python -m src.pipeline.pipeline --config config/mutag.yaml`) beforehand so that checkpoints exist for all components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment and configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:  # fallback for minimal environments\n",
    "    pd = None\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    plt = None\n",
    "    sns = None\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    np = None\n",
    "\n",
    "PROJECT_ROOT = Path(\"../\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "CONFIG_PATH = PROJECT_ROOT / \"config\" / \"mutag.yaml\"\n",
    "CHECKPOINT_ROOT = PROJECT_ROOT / \"checkpoints\"\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Using config: {CONFIG_PATH}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "seed = cfg.get(\"experiment\", {}).get(\"seed\", 42)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if torch.cuda.is_available() and cfg.get(\"experiment\", {}).get(\"device\", \"cuda\") == \"cuda\":\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Random seed set to {seed}\")\n",
    "print(f\"Evaluation device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    from torch_geometric.utils import to_networkx\n",
    "except ImportError as exc:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"torch_geometric is required for this notebook. Install it before proceeding.\"\n",
    "    ) from exc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from src.datasets.mutag import load_mutag\n",
    "from src.datasets.ged_dataset import GEDDataset, collate_pairs, ensure_ged_dataset\n",
    "from src.models.explainee_gnn import ExplaineeGIN\n",
    "from src.models.sim_gnn import SimGNN\n",
    "from src.models.generator import GraphGenerator\n",
    "from src.models.adapter import GeneratorAdapter\n",
    "from src.models.losses import (\n",
    "    SoftContrastiveEmbedLoss,\n",
    "    PredictionConfidenceLoss,\n",
    "    EdgePenalty,\n",
    "    _build_data_from_gen_output,\n",
    ")\n",
    "from src.utils.embeddings import compute_classwise_means\n",
    "from src.trainers.train_distance import evaluate as evaluate_distance_model\n",
    "\n",
    "\n",
    "def prepare_mutag_splits(test_ratio: float = 0.2, batch_size: int = 32, shuffle: bool = True, seed: int = 42):\n",
    "    dataset = load_mutag(root=str(DATA_ROOT / \"MUTAG\"))\n",
    "    labels = [int(d.y) for d in dataset]\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=test_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    train_data = [dataset[i] for i in train_idx]\n",
    "    test_data = [dataset[i] for i in test_idx]\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return dataset, train_loader, test_loader\n",
    "\n",
    "\n",
    "def dataset_summary(dataset) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for graph in dataset:\n",
    "        num_nodes = graph.num_nodes\n",
    "        num_edges = graph.edge_index.size(1)\n",
    "        rows.append({\n",
    "            \"num_nodes\": int(num_nodes),\n",
    "            \"num_edges\": int(num_edges),\n",
    "            \"label\": int(graph.y.item()) if hasattr(graph, \"y\") else None,\n",
    "        })\n",
    "    df = pd.DataFrame(rows) if pd is not None else rows\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset, train_loader, test_loader = prepare_mutag_splits(\n",
    "    test_ratio=0.2,\n",
    "    batch_size=cfg[\"explainee\"].get(\"batch_size\", 32),\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "print(f\"Loaded MUTAG dataset with {len(dataset)} graphs\")\n",
    "print(f\"Train loader batches: {len(train_loader)} | Test loader batches: {len(test_loader)}\")\n",
    "\n",
    "mutag_stats = dataset_summary(dataset)\n",
    "if pd is not None:\n",
    "    display(mutag_stats.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load explainee and evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_explainee(dataset, cfg_explainee: Dict, device: torch.device) -> ExplaineeGIN:\n",
    "    in_dim = dataset[0].x.size(1)\n",
    "    model = ExplaineeGIN(\n",
    "        in_dim=in_dim,\n",
    "        hidden_dim=cfg_explainee.get(\"hidden_dim\", 32),\n",
    "        num_layers=cfg_explainee.get(\"num_layers\", 2),\n",
    "        dropout=cfg_explainee.get(\"dropout\", 0.2),\n",
    "        num_classes=cfg_explainee.get(\"num_classes\", cfg[\"dataset\"].get(\"num_classes\", 2)),\n",
    "    ).to(device)\n",
    "\n",
    "    ckpt_path = PROJECT_ROOT / cfg_explainee.get(\"save_path\", \"models/explainees/gin_mutag.pt\")\n",
    "    if ckpt_path.exists():\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        print(f\"Loaded explainee checkpoint from {ckpt_path}\")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f Explainee checkpoint missing: {ckpt_path}. Using randomly initialised weights.\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_explainee(model: ExplaineeGIN, loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)\n",
    "            loss = F.cross_entropy(logits, batch.y, reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "            total += batch.y.size(0)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(batch.y.cpu())\n",
    "\n",
    "    logits = torch.cat(all_logits)\n",
    "    labels = torch.cat(all_labels)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(total, 1),\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, average='weighted', zero_division=0),\n",
    "    }\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    metrics_df = pd.DataFrame(metrics, index=[\"Explainee\"]) if pd is not None else metrics\n",
    "    if pd is not None:\n",
    "        display(metrics_df)\n",
    "        display(pd.DataFrame(cm, index=[\"true_0\", \"true_1\"], columns=[\"pred_0\", \"pred_1\"]))\n",
    "    else:\n",
    "        print(metrics)\n",
    "        print(\"Confusion matrix:\n",
    "\", cm)\n",
    "\n",
    "    return {\n",
    "        **metrics,\n",
    "        \"logits\": logits,\n",
    "        \"labels\": labels,\n",
    "        \"probs\": probs,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainee_cfg = cfg.get(\"explainee\", {})\n",
    "explainee_model = build_explainee(dataset, explainee_cfg, device)\n",
    "explainee_eval = evaluate_explainee(explainee_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SimGNN distance model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_distance_model(cfg_distance: Dict, dataset, device: torch.device):\n",
    "    ged_path = cfg_distance.get(\"data_path\", \"data/mutag_ged.pt\")\n",
    "    ged_path = PROJECT_ROOT / ged_path\n",
    "    if not ged_path.exists():\n",
    "        print(f\"Generating GED pairs at {ged_path} ...\")\n",
    "        ensure_ged_dataset(list(dataset), out_path=str(ged_path), alpha=cfg_distance.get(\"alpha\", 0.5))\n",
    "\n",
    "    ged_dataset = GEDDataset.load(ged_path)\n",
    "    loader = DataLoader(ged_dataset, batch_size=cfg_distance.get(\"batch_size\", 32), shuffle=False, collate_fn=collate_pairs)\n",
    "\n",
    "    in_dim = ged_dataset[0][0].x.size(1)\n",
    "    model = SimGNN(\n",
    "        in_dim,\n",
    "        hidden_dim=cfg_distance.get(\"hidden_dim\", 64),\n",
    "        tensor_channels=cfg_distance.get(\"tensor_channels\", 8),\n",
    "        use_tensor=cfg_distance.get(\"use_tensor\", True),\n",
    "    ).to(device)\n",
    "\n",
    "    ckpt_path = PROJECT_ROOT / cfg_distance.get(\"save_path\", \"models/distances/simgnn_mutag.pt\")\n",
    "    if ckpt_path.exists():\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        print(f\"Loaded SimGNN checkpoint from {ckpt_path}\")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f Distance model checkpoint missing: {ckpt_path}. Using randomly initialised weights.\")\n",
    "    model.eval()\n",
    "    return model, ged_dataset, loader\n",
    "\n",
    "\n",
    "def summarize_distance_metrics(model: SimGNN, loader, device: torch.device) -> Dict[str, float]:\n",
    "    metrics = evaluate_distance_model(model, loader, device)\n",
    "    metrics_df = pd.DataFrame(metrics, index=[\"SimGNN\"]) if pd is not None else metrics\n",
    "    if pd is not None:\n",
    "        display(metrics_df)\n",
    "    else:\n",
    "        print(metrics)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_cfg = cfg.get(\"distance_model\", {})\n",
    "simgnn_model, ged_dataset, ged_loader = load_distance_model(distance_cfg, dataset, device)\n",
    "distance_metrics = summarize_distance_metrics(simgnn_model, ged_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generator loading and helper utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GeneratorArtifacts:\n",
    "    model: GraphGenerator\n",
    "    state_path: Path\n",
    "    target_class: int\n",
    "\n",
    "\n",
    "def infer_generator_spec(dataset) -> Dict:\n",
    "    adapter = GeneratorAdapter(dataset)\n",
    "    spec = {\n",
    "        \"max_nodes\": adapter.max_nodes,\n",
    "        \"num_cont_node_feats\": adapter.num_cont_node_feats,\n",
    "        \"dis_node_blocks\": adapter.dis_node_blocks,\n",
    "        \"num_cont_edge_feats\": adapter.num_cont_edge_feats,\n",
    "        \"dis_edge_blocks\": adapter.dis_edge_blocks,\n",
    "    }\n",
    "    print(\"Generator spec:\", spec)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def load_generators(cfg_gen: Dict, dataset, device: torch.device):\n",
    "    spec = infer_generator_spec(dataset)\n",
    "    save_template = cfg_gen.get(\"save_path\", \"checkpoints/generators/generator_classX.pt\")\n",
    "    fallback_template = None\n",
    "    if \"generator-class\" in save_template:\n",
    "        fallback_template = save_template.replace(\"generator-class\", \"generator_class\")\n",
    "    artifacts = []\n",
    "    num_classes = cfg[\"dataset\"].get(\"num_classes\", 2)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        template = save_template.replace(\"X\", str(class_idx))\n",
    "        path = PROJECT_ROOT / template\n",
    "        resolved_path = path if path.exists() else None\n",
    "        checked_paths = [path]\n",
    "        if resolved_path is None and fallback_template is not None:\n",
    "            alt_template = fallback_template.replace(\"X\", str(class_idx))\n",
    "            alt_path = PROJECT_ROOT / alt_template\n",
    "            checked_paths.append(alt_path)\n",
    "            if alt_path.exists():\n",
    "                print(f\"Fallback: resolved generator checkpoint for class {class_idx} to {alt_path}\")\n",
    "                resolved_path = alt_path\n",
    "        gen = GraphGenerator(\n",
    "            batch_size=cfg_gen.get(\"batch_size\", 1),\n",
    "            temperature=cfg_gen.get(\"temperature\", 1.0),\n",
    "            **spec,\n",
    "        ).to(device)\n",
    "        if resolved_path is not None and resolved_path.exists():\n",
    "            state = torch.load(resolved_path, map_location=device)\n",
    "            gen.load_state_dict(state)\n",
    "            print(f\"Loaded generator for class {class_idx} from {resolved_path}\")\n",
    "        else:\n",
    "            missing_list = \", \".join(str(p) for p in checked_paths)\n",
    "            print(f\"\u26a0\ufe0f Generator checkpoint missing for class {class_idx}. Checked: {missing_list}\")\n",
    "        gen.eval()\n",
    "        artifacts.append(\n",
    "            GeneratorArtifacts(model=gen, state_path=resolved_path if resolved_path is not None else path, target_class=class_idx)\n",
    "        )\n",
    "    return artifacts\n",
    "\n",
    "\n",
    "generator_cfg = cfg.get(\"generator\", {})\n",
    "generators = load_generators(generator_cfg, dataset, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-wise embedding means\n",
    "\n",
    "The embedding loss requires per-layer class means from the explainee. We recompute them using the training split to avoid leaking test labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classwise_means = compute_classwise_means(\n",
    "    model=explainee_model,\n",
    "    dataloader=train_loader,\n",
    "    device=device,\n",
    "    num_classes=cfg[\"dataset\"].get(\"num_classes\", 2),\n",
    ")\n",
    "print(f\"Computed means for {len(classwise_means)} layers: {list(classwise_means.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generator metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric glossary\n",
    "\n",
    "The tables below use the following quantities:\n",
    "\n",
    "* **Explainee metrics** \u2013 `accuracy`, `precision`, `recall`, and `f1` measure how well the explainee model predicts the held-out MUTAG labels. The `confusion_matrix` highlights per-class confusions.\n",
    "* **SimGNN distance metrics** \u2013 `mae_norm`/`mse_norm` report absolute and squared errors on normalised GED targets, while `mae_raw`/`mse_raw` do the same on the unnormalised distances. `corr_norm`/`corr_raw` give the Pearson correlation between SimGNN predictions and the ground-truth GED scores.\n",
    "* **Generator loss terms** \u2013 `total` aggregates the embedding (`pull`+`push`), prediction (`pred`), and structural (`edge`) penalties used during training. Lower values indicate the generator matches the explainee prototype with fewer regularisation violations.\n",
    "* **Fidelity metrics** \u2013 `confidence_mean` is the explainee's average probability for the target class, `margin_mean` is the gap to the runner-up class, and `entropy_mean` captures output uncertainty. Higher confidence and margin with lower entropy reflect explanations aligned with the explainee.\n",
    "* **Structural summaries** \u2013 `num_nodes`, `num_edges`, `avg_degree`, and `density` describe the generated graphs' size and connectivity, mirroring the statistics reported for the observed MUTAG graphs.\n",
    "* **GED alignment** \u2013 `ged_min` is the closest SimGNN-estimated distance to any real graph of the class, while `ged_mean` is the average distance across the comparison set. Lower values imply better structural faithfulness.\n",
    "* **Diversity** \u2013 the average pairwise SimGNN distance between generated samples, indicating how varied the generator outputs are for a class.\n",
    "* **Observed graph statistics** \u2013 the report also lists dataset-wide counts and summary stats for real MUTAG graphs to contextualise the generator outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def generator_forward_samples(gen: GraphGenerator, num_samples: int = 16):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            out = gen()\n",
    "            outputs.append({k: v.detach().cpu() for k, v in out.items()})\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def logits_from_gen_output(gen_out, explainee: torch.nn.Module, thresh: float = 0.5) -> torch.Tensor:\n",
    "    batch = _build_data_from_gen_output({k: v.to(device) for k, v in gen_out.items()}, thresh=thresh)\n",
    "    logits = explainee(batch)\n",
    "    return logits.detach().cpu()\n",
    "\n",
    "\n",
    "def build_data_from_output(gen_out, thresh: float = 0.5):\n",
    "    batch = _build_data_from_gen_output({k: v.to(device) for k, v in gen_out.items()}, thresh=thresh)\n",
    "    data_list = batch.to_data_list()\n",
    "    assert len(data_list) == 1, \"Generator batch size > 1 not supported yet\"\n",
    "    data = data_list[0].cpu()\n",
    "    data.edge_index = data.edge_index.long()\n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_loss_terms(gen_out, target_class: int, lambda_pred: float, lambda_edge: float):\n",
    "    emb_loss = SoftContrastiveEmbedLoss(\n",
    "        explainee=explainee_model,\n",
    "        classwise_means=classwise_means,\n",
    "        target_class=target_class,\n",
    "        layer_names=classwise_means.keys(),\n",
    "    )\n",
    "    pred_loss = PredictionConfidenceLoss(explainee_model, target_class=target_class)\n",
    "    edge_pen = EdgePenalty()\n",
    "\n",
    "    out_device = {k: v.to(device) for k, v in gen_out.items()}\n",
    "    comps = emb_loss.forward_components(out_device)\n",
    "    l_pred = pred_loss(out_device)\n",
    "    l_edge = edge_pen(out_device[\"adj\"])\n",
    "    total = (comps[\"pull\"] + comps[\"push\"]) + lambda_pred * l_pred + lambda_edge * l_edge\n",
    "    return {\n",
    "        \"total\": float(total.cpu()),\n",
    "        \"pull\": float(comps[\"pull\"].cpu()),\n",
    "        \"push\": float(comps[\"push\"].cpu()),\n",
    "        \"pred\": float(l_pred.cpu()),\n",
    "        \"edge\": float(l_edge.cpu()),\n",
    "    }\n",
    "\n",
    "\n",
    "def graph_stats(data: Data):\n",
    "    num_nodes = int(data.num_nodes)\n",
    "    num_edges = int(data.edge_index.size(1) // 2) if data.edge_index.size(1) > 0 else 0\n",
    "    deg = torch.bincount(data.edge_index[0], minlength=num_nodes).float() if data.edge_index.numel() > 0 else torch.zeros(num_nodes)\n",
    "    return {\n",
    "        \"num_nodes\": num_nodes,\n",
    "        \"num_edges\": num_edges,\n",
    "        \"avg_degree\": float(deg.mean().item()) if deg.numel() > 0 else 0.0,\n",
    "        \"density\": float(num_edges) / max(num_nodes * (num_nodes - 1) / 2, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def fidelity_metrics(logits: torch.Tensor, target_class: int):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    target_probs = probs[:, target_class]\n",
    "    top2 = probs.topk(2, dim=1).values\n",
    "    margins = target_probs - top2[:, 1]\n",
    "    entropy = -(probs * probs.clamp_min(1e-9).log()).sum(dim=1)\n",
    "    return {\n",
    "        \"confidence_mean\": float(target_probs.mean()),\n",
    "        \"confidence_std\": float(target_probs.std(unbiased=False)) if target_probs.numel() > 1 else 0.0,\n",
    "        \"margin_mean\": float(margins.mean()),\n",
    "        \"entropy_mean\": float(entropy.mean()),\n",
    "    }\n",
    "\n",
    "\n",
    "def distance_to_real_samples(data: Data, real_graphs: List[Data], distance_model: SimGNN, device: torch.device):\n",
    "    if not real_graphs:\n",
    "        return {\"ged_min\": float(\"nan\"), \"ged_mean\": float(\"nan\")}\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        batch_fake = Batch.from_data_list([data]).to(device)\n",
    "        for real in real_graphs:\n",
    "            batch_real = Batch.from_data_list([real]).to(device)\n",
    "            pred = distance_model(batch_fake, batch_real)\n",
    "            scores.append(float(pred.cpu()))\n",
    "    return {\n",
    "        \"ged_min\": min(scores) if scores else float(\"nan\"),\n",
    "        \"ged_mean\": float(sum(scores) / len(scores)) if scores else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def pairwise_diversity(datas: List[Data], distance_model: SimGNN, device: torch.device) -> float:\n",
    "    if len(datas) < 2:\n",
    "        return float(\"nan\")\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i, j in combinations(range(len(datas)), 2):\n",
    "            b_i = Batch.from_data_list([datas[i]]).to(device)\n",
    "            b_j = Batch.from_data_list([datas[j]]).to(device)\n",
    "            pred = distance_model(b_i, b_j)\n",
    "            scores.append(float(pred.cpu()))\n",
    "    return float(sum(scores) / len(scores)) if scores else float(\"nan\")\n",
    "\n",
    "\n",
    "def evaluate_generator(artifact, cfg_gen: Dict, real_dataset, distance_model: SimGNN, device: torch.device, num_samples: int = 16):\n",
    "    outputs = generator_forward_samples(artifact.model, num_samples=num_samples)\n",
    "    lambda_pred = cfg_gen.get(\"lambda_pred\", 1.0)\n",
    "    lambda_edge = cfg_gen.get(\"lambda_edge\", 0.1)\n",
    "\n",
    "    real_subset = [g for g in real_dataset if int(g.y.item()) == artifact.target_class]\n",
    "    real_subset = random.sample(real_subset, min(len(real_subset), 32)) if len(real_subset) > 32 else real_subset\n",
    "\n",
    "    rows = []\n",
    "    datas = []\n",
    "    for idx, out in enumerate(outputs):\n",
    "        logits = logits_from_gen_output(out, explainee_model)\n",
    "        data = build_data_from_output(out)\n",
    "        datas.append(data)\n",
    "\n",
    "        loss_terms = compute_loss_terms(out, artifact.target_class, lambda_pred=lambda_pred, lambda_edge=lambda_edge)\n",
    "        fid = fidelity_metrics(logits, artifact.target_class)\n",
    "        stats = graph_stats(data)\n",
    "        ged = distance_to_real_samples(data, real_subset, distance_model, device)\n",
    "\n",
    "        row = {\n",
    "            \"sample\": idx,\n",
    "            **loss_terms,\n",
    "            **fid,\n",
    "            **stats,\n",
    "            **ged,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    diversity = pairwise_diversity(datas, distance_model, device)\n",
    "    summary = pd.DataFrame(rows) if pd is not None else rows\n",
    "    if pd is not None:\n",
    "        summary.attrs[\"diversity\"] = diversity\n",
    "    return summary, diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator_results = {}\n",
    "for artifact in generators:\n",
    "    summary_df, diversity = evaluate_generator(artifact, generator_cfg, dataset, simgnn_model, device, num_samples=16)\n",
    "    generator_results[artifact.target_class] = summary_df\n",
    "    if pd is not None and isinstance(summary_df, pd.DataFrame) and not summary_df.empty:\n",
    "        display(summary_df.describe())\n",
    "        print(f\"Pairwise diversity (SimGNN GED) for class {artifact.target_class}: {diversity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visual inspection of generated graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph_pair(gen_data: Data, real_data: Data, title: str):\n",
    "    if plt is None:\n",
    "        print(\"matplotlib is not available; skipping visualization.\")\n",
    "        return\n",
    "    import networkx as nx\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    G_gen = to_networkx(gen_data, to_undirected=True)\n",
    "    G_real = to_networkx(real_data, to_undirected=True)\n",
    "    pos_gen = nx.spring_layout(G_gen, seed=seed)\n",
    "    pos_real = nx.spring_layout(G_real, seed=seed)\n",
    "\n",
    "    axes[0].set_title(f\"Generated ({title})\")\n",
    "    nx.draw_networkx(G_gen, pos=pos_gen, ax=axes[0], with_labels=False, node_size=150)\n",
    "\n",
    "    axes[1].set_title(\"Nearest real\")\n",
    "    nx.draw_networkx(G_real, pos=pos_real, ax=axes[1], with_labels=False, node_size=150, node_color='lightgrey')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "for cls, df in generator_results.items():\n",
    "    if pd is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        continue\n",
    "    best_idx = df[\"ged_min\"].idxmin()\n",
    "    samples, _ = evaluate_generator(generators[cls], generator_cfg, dataset, simgnn_model, device, num_samples=best_idx + 1)\n",
    "    if pd is not None and isinstance(samples, pd.DataFrame):\n",
    "        out = generator_forward_samples(generators[cls].model, num_samples=best_idx + 1)[best_idx]\n",
    "    else:\n",
    "        out = generator_forward_samples(generators[cls].model, num_samples=best_idx + 1)[best_idx]\n",
    "    gen_data = build_data_from_output(out)\n",
    "    target_graphs = [g for g in dataset if int(g.y.item()) == cls]\n",
    "    real_subset = random.sample(target_graphs, min(len(target_graphs), 32)) if len(target_graphs) > 32 else target_graphs\n",
    "    best_real = None\n",
    "    best_score = float(\"inf\")\n",
    "    with torch.no_grad():\n",
    "        batch_fake = Batch.from_data_list([gen_data]).to(device)\n",
    "        for real in real_subset:\n",
    "            batch_real = Batch.from_data_list([real]).to(device)\n",
    "            score = float(simgnn_model(batch_fake, batch_real).cpu())\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_real = real\n",
    "    if best_real is not None:\n",
    "        fig = plot_graph_pair(gen_data, best_real, title=f\"class {cls}\")\n",
    "        if plt is not None:\n",
    "            plt.suptitle(f\"Class {cls} | SimGNN GED \u2248 {best_score:.3f}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parameter_summary(model: torch.nn.Module):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\"total_params\": total, \"trainable_params\": trainable}\n",
    "\n",
    "\n",
    "param_rows = []\n",
    "param_rows.append({\"component\": \"Explainee\", **parameter_summary(explainee_model)})\n",
    "param_rows.append({\"component\": \"SimGNN\", **parameter_summary(simgnn_model)})\n",
    "for art in generators:\n",
    "    summary = parameter_summary(art.model)\n",
    "    summary.update({\n",
    "        \"component\": f\"Generator_class_{art.target_class}\",\n",
    "        \"state_path\": str(art.state_path),\n",
    "        \"adj_min\": float(art.model.adj_logits.min().item()),\n",
    "        \"adj_max\": float(art.model.adj_logits.max().item()),\n",
    "    })\n",
    "    param_rows.append(summary)\n",
    "\n",
    "param_df = pd.DataFrame(param_rows) if pd is not None else param_rows\n",
    "if pd is not None:\n",
    "    display(param_df)\n",
    "else:\n",
    "    print(param_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Consolidated report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    \"explainee\": {k: float(v) for k, v in explainee_eval.items() if isinstance(v, (int, float))},\n",
    "    \"distance\": {k: float(v) for k, v in distance_metrics.items()},\n",
    "    \"observed_graphs\": {},\n",
    "    \"generators\": {},\n",
    "}\n",
    "\n",
    "observed_stats = [graph_stats(graph) for graph in dataset]\n",
    "node_counts = [stats[\"num_nodes\"] for stats in observed_stats]\n",
    "edge_counts = [stats[\"num_edges\"] for stats in observed_stats]\n",
    "avg_degrees = [stats[\"avg_degree\"] for stats in observed_stats]\n",
    "densities = [stats[\"density\"] for stats in observed_stats]\n",
    "labels = [int(graph.y.item()) if hasattr(graph, \"y\") else None for graph in dataset]\n",
    "label_counter = Counter(label for label in labels if label is not None)\n",
    "\n",
    "def summarise(values):\n",
    "    if not values:\n",
    "        return {\"mean\": float(\"nan\"), \"std\": float(\"nan\"), \"min\": float(\"nan\"), \"max\": float(\"nan\")}\n",
    "    mean = sum(values) / len(values)\n",
    "    variance = sum((val - mean) ** 2 for val in values) / len(values)\n",
    "    return {\n",
    "        \"mean\": float(mean),\n",
    "        \"std\": float(variance ** 0.5),\n",
    "        \"min\": float(min(values)),\n",
    "        \"max\": float(max(values)),\n",
    "    }\n",
    "\n",
    "report[\"observed_graphs\"] = {\n",
    "    \"count\": len(dataset),\n",
    "    \"label_counts\": {int(k): int(v) for k, v in label_counter.items()},\n",
    "    \"num_nodes\": summarise(node_counts),\n",
    "    \"num_edges\": summarise(edge_counts),\n",
    "    \"avg_degree\": summarise(avg_degrees),\n",
    "    \"density\": summarise(densities),\n",
    "}\n",
    "\n",
    "for cls, df in generator_results.items():\n",
    "    if pd is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        continue\n",
    "\n",
    "    def floatify(mapping):\n",
    "        return {k: float(v) if v == v else float(\"nan\") for k, v in mapping.items()}\n",
    "\n",
    "    report[\"generators\"][f\"class_{cls}\"] = {\n",
    "        \"loss\": floatify(df[[\"total\", \"pull\", \"push\", \"pred\", \"edge\"]].mean().to_dict()),\n",
    "        \"fidelity\": floatify(df[[\"confidence_mean\", \"margin_mean\", \"entropy_mean\"]].mean().to_dict()),\n",
    "        \"structure\": floatify(df[[\"num_nodes\", \"num_edges\", \"avg_degree\", \"density\"]].mean().to_dict()),\n",
    "        \"ged\": floatify(df[[\"ged_min\", \"ged_mean\"]].mean().to_dict()),\n",
    "        \"diversity\": float(df.attrs.get(\"diversity\")) if df.attrs.get(\"diversity\") is not None else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "print(json.dumps(report, indent=2, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Inspect cases where generator confidence is low but pull/push losses are small \u2014 this may indicate insufficient prediction guidance.\n",
    "* Compare GED statistics against real validation graphs to ensure the distance model generalises.\n",
    "* Use the diversity metric to tune temperature or regularisation if the generator collapses to a single prototype.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}